{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3dd98390-6cf1-4188-8828-d9bbbeea1bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23eb670f-5896-440f-9bd5-2b5bc312537b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b49ed73-71d3-4ced-80bf-7e486e04fbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name='ml-100k'\n",
    "dataset_folder='ml'\n",
    "preflix_folder='11_17_new_settings'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4a9ec17-4926-453d-8ca9-9113908b350d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "windows\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "if platform.system().lower() == 'windows':\n",
    "    import pickle5 as pickle\n",
    "    with open('E:/datasets/'+dataset_folder+'/'+dataset_name+'/'+preflix_folder+'/'+dataset_name+'_all_ratings_mapped_index.pkl', 'rb') as f:  \n",
    "        user_dict = pickle.load(f)\n",
    "    print(\"windows\")\n",
    "elif platform.system().lower() == 'linux':\n",
    "    import pickle\n",
    "    with open('/root/autodl-nas/workspace/datasets/'+dataset_folder+'/'+dataset_name+'/'+preflix_folder+'/'+dataset_name+'_all_ratings_mapped_index.pkl', 'rb') as f:  \n",
    "        user_dict = pickle.load(f)\n",
    "    print(\"linux\")\n",
    "items_num=[]\n",
    "for i in user_dict.values():\n",
    "    items_num.append(max(i))\n",
    "items_num=max(items_num)\n",
    "\n",
    "len_seq=10\n",
    "\n",
    "\n",
    "batch_size=512\n",
    "epoch_num=100\n",
    "hidden_size=64\n",
    "keep_rate=0.9\n",
    "layers_num=3\n",
    "\n",
    "interest_num=3\n",
    "kernel_size=4\n",
    "transformerencoder=0\n",
    "\n",
    "neg_num=5\n",
    "test_neg_num=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4abddbd-3124-41e1-a15c-16c9ae07e1b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "windows\n"
     ]
    }
   ],
   "source": [
    "if platform.system().lower() == 'windows':\n",
    "    print(\"windows\")\n",
    "    with open('E:/datasets/'+dataset_folder+'/'+dataset_name+'/'+preflix_folder+'/'+dataset_name+'_'+str(len_seq)+'_seq_train_data.pkl', 'rb') as f:\n",
    "        train_data=pickle.load(f)\n",
    "    with open('E:/datasets/'+dataset_folder+'/'+dataset_name+'/'+preflix_folder+'/'+dataset_name+'_'+str(len_seq)+'_seq_test_data.pkl', 'rb') as f:\n",
    "        test_data=pickle.load(f)\n",
    "    with open('E:/datasets/'+dataset_folder+'/'+dataset_name+'/'+preflix_folder+'/'+dataset_name+'_'+str(len_seq)+'_seq_val_data.pkl', 'rb') as f:\n",
    "        val_data=pickle.load(f)\n",
    "elif platform.system().lower() == 'linux':\n",
    "    print(\"linux\")\n",
    "    with open('/root/autodl-nas/workspace/datasets/'+dataset_folder+'/'+dataset_name+'/'+preflix_folder+'/'+dataset_name+'_'+str(len_seq)+'_seq_train_data.pkl', 'rb') as f:\n",
    "        train_data=pickle.load(f)\n",
    "    with open('/root/autodl-nas/workspace/datasets/'+dataset_folder+'/'+dataset_name+'/'+preflix_folder+'/'+dataset_name+'_'+str(len_seq)+'_seq_test_data.pkl', 'rb') as f:\n",
    "        test_data=pickle.load(f)\n",
    "    with open('/root/autodl-nas/workspace/datasets/'+dataset_folder+'/'+dataset_name+'/'+preflix_folder+'/'+dataset_name+'_'+str(len_seq)+'_seq_val_data.pkl', 'rb') as f:\n",
    "        val_data=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32879739-778f-4a8b-aabe-f2b701d2253b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(51854, 10) (6969, 10) (14264, 10)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(train_data['click_seq']),np.shape(val_data['click_seq']),np.shape(test_data['click_seq']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1df82d54-78b8-48b8-84b9-ad4896e2236a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    \"\"\"Attention Mechanism Function.\n",
    "    Args:\n",
    "        :param q: A 3d/4d tensor with shape of (None, ..., seq_len, dim)\n",
    "        :param k: A 3d/4d tensor with shape of (None, ..., seq_len, dim)\n",
    "        :param v: A 3d/4d tensor with shape of (None, ..., seq_len, dim)\n",
    "        :param mask: A 3d/4d tensor with shape of (None, ..., seq_len, 1)\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    mat_qk = tf.matmul(q, k, transpose_b=True)  # (None, seq_len, seq_len)\n",
    "    # Scaled\n",
    "    dk = tf.cast(k.shape[-1], dtype=tf.float32)\n",
    "    scaled_att_logits = mat_qk / tf.sqrt(dk)\n",
    "\n",
    "    paddings = tf.ones_like(scaled_att_logits) * (-2 ** 32 + 1)  # (None, seq_len, seq_len)\n",
    "    if mask!=None:\n",
    "        outputs = tf.where(tf.equal(mask, tf.zeros_like(mask)), paddings, scaled_att_logits)  # (None, seq_len, seq_len)\n",
    "    else:\n",
    "        outputs=scaled_att_logits\n",
    "    # softmax\n",
    "    outputs = tf.nn.softmax(logits=outputs)  # (None, seq_len, seq_len)\n",
    "    outputs = tf.matmul(outputs, v)  # (None, seq_len, dim)\n",
    "\n",
    "    return outputs\n",
    "\n",
    "\n",
    "def split_heads(x, seq_len, num_heads, depth):\n",
    "    \"\"\"Split the last dimension into (num_heads, depth).\n",
    "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "    Args:\n",
    "        :param x: A Tensor with shape of [batch_size, seq_len, num_heads * depth]\n",
    "        :param seq_len: A scalar(int).\n",
    "        :param num_heads: A scalar(int).\n",
    "        :param depth: A scalar(int).\n",
    "    :return: A tensor with shape of [batch_size, num_heads, seq_len, depth]\n",
    "    \"\"\"\n",
    "    x = tf.reshape(x, (-1, seq_len, num_heads, depth))\n",
    "    return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "def normalize_adj_tensor(adj, seq_len):\n",
    "    adj = adj + tf.expand_dims(tf.eye(seq_len), axis=0)\n",
    "    rowsum = tf.reduce_sum(adj, axis=1)\n",
    "    d_inv_sqrt = tf.pow(rowsum, -0.5)\n",
    "    candidate_a = tf.zeros_like(d_inv_sqrt)\n",
    "    d_inv_sqrt = tf.where(tf.math.is_inf(d_inv_sqrt), candidate_a, d_inv_sqrt)\n",
    "    d_mat_inv_sqrt = tf.linalg.diag(d_inv_sqrt)\n",
    "    norm_adg = tf.matmul(d_mat_inv_sqrt, adj)\n",
    "    return norm_adg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1382056-672c-4683-a46a-dca40cd582b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        \"\"\"Multi Head Attention Mechanism.\n",
    "        Args:\n",
    "            :param d_model: A scalar. The self-attention hidden size.\n",
    "            :param num_heads: A scalar. Number of heads. If num_heads == 1, the layer is a single self-attention layer.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.wq = tf.keras.layers.Dense(d_model, activation=None)\n",
    "        self.wk = tf.keras.layers.Dense(d_model, activation=None)\n",
    "        self.wv = tf.keras.layers.Dense(d_model, activation=None)\n",
    "\n",
    "    def call(self, q, k, v, mask):\n",
    "        q = self.wq(q)  # (None, seq_len, d_model)\n",
    "        k = self.wk(k)  # (None, seq_len, d_model)\n",
    "        v = self.wv(v)  # (None, seq_len, d_model)\n",
    "        # split d_model into num_heads * depth\n",
    "        seq_len, d_model = q.shape[1], q.shape[2]\n",
    "        q = split_heads(q, seq_len, self.num_heads, q.shape[2] // self.num_heads)  # (None, num_heads, seq_len, depth)\n",
    "        k = split_heads(k, seq_len, self.num_heads, k.shape[2] // self.num_heads)  # (None, num_heads, seq_len, depth)\n",
    "        v = split_heads(v, seq_len, self.num_heads, v.shape[2] // self.num_heads)  # (None, num_heads, seq_len, depth)\n",
    "        # mask\n",
    "        if mask!=None:\n",
    "            mask = tf.tile(tf.expand_dims(mask, axis=1), [1, self.num_heads, 1, 1])  # (None, num_heads, seq_len, 1)\n",
    "        # attention\n",
    "        scaled_attention = scaled_dot_product_attention(q, k, v, mask)  # (None, num_heads, seq_len, d_model // num_heads)\n",
    "        # reshape\n",
    "        outputs = tf.reshape(tf.transpose(scaled_attention, [0, 2, 1, 3]), [-1, seq_len, d_model])  # (None, seq_len, d_model)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class FFN(tf.keras.layers.Layer):\n",
    "    def __init__(self, hidden_unit, d_model):\n",
    "        \"\"\"Feed Forward Network.\n",
    "        Args:\n",
    "            :param hidden_unit: A scalar.\n",
    "            :param d_model: A scalar.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        super(FFN, self).__init__()\n",
    "        self.conv1 = tf.keras.layers.Conv1D(filters=hidden_unit, kernel_size=1, activation='relu', use_bias=True)\n",
    "        self.conv2 = tf.keras.layers.Conv1D(filters=d_model, kernel_size=1, activation=None, use_bias=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "        output = self.conv2(x)\n",
    "        return output\n",
    "\n",
    "\n",
    "class TransformerEncoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads=1, ffn_hidden_unit=128, dropout=0., layer_norm_eps=1e-6):\n",
    "        \"\"\"Encoder Layer.\n",
    "        Args:\n",
    "            :param d_model: A scalar. The self-attention hidden size.\n",
    "            :param num_heads: A scalar. Number of heads.\n",
    "            :param ffn_hidden_unit: A scalar. Number of hidden unit in FFN\n",
    "            :param dropout: A scalar. Number of dropout.\n",
    "            :param layer_norm_eps: A scalar. Small float added to variance to avoid dividing by zero.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = FFN(ffn_hidden_unit, d_model)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=layer_norm_eps)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=layer_norm_eps)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(dropout)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x, mask= inputs\n",
    "        # self-attention\n",
    "        att_out = self.mha(x, x, x, mask)  # (None, seq_len, d_model)\n",
    "        att_out = self.dropout1(att_out)\n",
    "        # residual add\n",
    "        out1 = self.layernorm1(x + att_out)  # (None, seq_len, d_model)\n",
    "        # ffn\n",
    "        ffn_out = self.ffn(out1)\n",
    "        ffn_out = self.dropout2(ffn_out)\n",
    "        # residual add\n",
    "        out2 = self.layernorm2(out1 + ffn_out)  # (None, seq_len, d_model)\n",
    "        return out2\n",
    "\n",
    "class gcn_layer(tf.keras.layers.Layer):\n",
    "    def __init__(self, hidden_size, num_layer):\n",
    "        super(gcn_layer, self).__init__()\n",
    "        #weights_size_list = [hidden_size,64,64,64]\n",
    "        #all_weights = {}\n",
    "        self.num_layer=num_layer\n",
    "        self.conv_dense=tf.keras.layers.Dense(64, activation=tf.keras.layers.LeakyReLU(alpha=0.01))\n",
    "    \n",
    "    def call(self,A, x):\n",
    "    # gcn has three layers\n",
    "        all_embeddings =[x]\n",
    "        for k in range(self.num_layer):\n",
    "            embeddings = tf.matmul(A, x)\n",
    "            embeddings = self.conv_dense(embeddings)\n",
    "            all_embeddings.append(embeddings)\n",
    "        return all_embeddings\n",
    "\n",
    "class light_gcn_layer(tf.keras.layers.Layer):\n",
    "    def __init__(self, hidden_size, num_layer):\n",
    "        super(gcn_layer, self).__init__()\n",
    "        #weights_size_list = [hidden_size,64,64,64]\n",
    "        #all_weights = {}\n",
    "        self.num_layer=num_layer\n",
    "        self.conv_dense=tf.keras.layers.Dense(64, activation=tf.keras.layers.LeakyReLU(alpha=0.01))\n",
    "    \n",
    "    def call(self,A, x):\n",
    "    # gcn has three layers\n",
    "        all_embeddings =[x]\n",
    "        for k in range(self.num_layer):\n",
    "            embeddings = tf.matmul(A, x)\n",
    "            #embeddings = self.conv_dense(embeddings)\n",
    "            all_embeddings.append(embeddings)\n",
    "        return all_embeddings    \n",
    "    \n",
    "class hierarchical_interest(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        ###-------------------------------###\n",
    "    def call(self, ori_all_embeddings):   \n",
    "        ###-------------------------------###\n",
    "        return all_embeddings\n",
    "\n",
    "class CapsuleNetwork(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, seq_len, bilinear_type=0, num_interest=4, stop_grad=True):\n",
    "        super(CapsuleNetwork, self).__init__()\n",
    "        self.bilinear_type = bilinear_type\n",
    "        self.seq_len = seq_len\n",
    "        self.num_interest = num_interest\n",
    "        self.embed_dim = embed_dim\n",
    "        self.bi_dense=tf.keras.layers.Dense(hidden_size * self.num_interest, activation=None,use_bias=False)\n",
    "        self.lstm=tf.keras.layers.LSTM(self.embed_dim*2, unit_forget_bias=1.0,return_sequences=True)\n",
    "        self.stop_grad = stop_grad\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        if self.bilinear_type >= 2:\n",
    "            self.w = self.add_weight(\n",
    "                shape=[1, self.seq_len, self.num_interest * self.embed_dim, self.embed_dim],\n",
    "                initializer='random_normal',\n",
    "                name='weights'\n",
    "            )\n",
    "\n",
    "    def call(self, hist_emb, mask):\n",
    "        if self.bilinear_type == 0:\n",
    "            hist_emb_hat = tf.tile(hist_emb, [1, 1, self.num_interest])  # (None, seq_len, num_inter * embed_dim)\n",
    "        elif self.bilinear_type == 1:\n",
    "            #outputs = self.lstm(hist_emb)\n",
    "            #output = tf.concat(outputs, axis=0)\n",
    "            #output = tf.reshape(output, (-1, self.seq_len, self.embed_dim*2))\n",
    "            #output = tf.layers.dense(output, self.embed_dim*self.num_interest, activation=None, use_bias=False)\n",
    "            hist_emb_hat = self.bi_dense(hist_emb)\n",
    "        else:\n",
    "            u = tf.expand_dims(hist_emb, axis=2)  # (None, seq_len, 1, embed_dim)\n",
    "            hist_emb_hat = tf.reduce_sum(self.w * u, axis=3)  # (None, seq_len, num_inter * embed_dim)\n",
    "        hist_emb_hat = tf.reshape(hist_emb_hat, [-1, self.seq_len, self.num_interest, self.embed_dim])\n",
    "        hist_emb_hat = tf.transpose(hist_emb_hat, [0, 2, 1, 3])  # (None, num_inter, seq_len, embed_dim)\n",
    "        hist_emb_hat = tf.reshape(hist_emb_hat, [-1, self.num_interest, self.seq_len, self.embed_dim])\n",
    "        if self.stop_grad:\n",
    "            hist_emb_iter = tf.stop_gradient(hist_emb_hat)\n",
    "        else:\n",
    "            hist_emb_iter = hist_emb_hat  # (None, num_inter, seq_len, embed_dim)\n",
    "\n",
    "        if self.bilinear_type > 0:\n",
    "            '''self.capsule_weight = self.add_weight(\n",
    "                shape=[tf.shape(hist_emb_hat)[0], self.num_interest, self.seq_len],\n",
    "                initializer=tf.zeros_initializer())'''\n",
    "            self.capsule_weight = tf.random.truncated_normal(\n",
    "                shape=[tf.shape(hist_emb_hat)[0], self.num_interest, self.seq_len],\n",
    "                stddev=1.0)\n",
    "            \n",
    "        else:\n",
    "            self.capsule_weight = tf.random.truncated_normal(\n",
    "                shape=[tf.shape(hist_emb_hat)[0], self.num_interest, self.seq_len],\n",
    "                stddev=1.0)\n",
    "        tf.stop_gradient(self.capsule_weight)\n",
    "\n",
    "        for i in range(3):\n",
    "            att_mask = tf.tile(tf.expand_dims(mask, axis=1), [1, self.num_interest, 1])  # (None, num_inter, seq_len)\n",
    "            paddings = tf.zeros_like(att_mask)\n",
    "\n",
    "            capsule_softmax_weight = tf.nn.softmax(self.capsule_weight, axis=1)  # (None, num_inter, seq_len)\n",
    "            capsule_softmax_weight = tf.where(tf.equal(att_mask, 0), paddings, capsule_softmax_weight)\n",
    "            capsule_softmax_weight = tf.expand_dims(capsule_softmax_weight, 2)  # (None, num_inter, 1, seq_len)\n",
    "\n",
    "            if i < 2:\n",
    "                interest_capsule = tf.matmul(capsule_softmax_weight, hist_emb_iter)  # (None, num_inter, 1, embed_dim)\n",
    "                cap_norm = tf.reduce_sum(tf.square(interest_capsule), -1, keepdims=True)\n",
    "                scalar_factor = cap_norm / (1 + cap_norm) / tf.sqrt(cap_norm + 1e-9)\n",
    "                interest_capsule = scalar_factor * interest_capsule\n",
    "\n",
    "                delta_weight = tf.matmul(hist_emb_iter, tf.transpose(interest_capsule, [0, 1, 3, 2]))\n",
    "                delta_weight = tf.reshape(delta_weight, [-1, self.num_interest, self.seq_len])\n",
    "                self.capsule_weight = self.capsule_weight + delta_weight\n",
    "            else:\n",
    "                interest_capsule = tf.matmul(capsule_softmax_weight, hist_emb_hat)\n",
    "                cap_norm = tf.reduce_sum(tf.square(interest_capsule), -1, True)\n",
    "                scalar_factor = cap_norm / (1 + cap_norm) / tf.sqrt(cap_norm + 1e-9)\n",
    "                interest_capsule = scalar_factor * interest_capsule\n",
    "\n",
    "        interest_capsule = tf.reshape(interest_capsule, [-1, self.num_interest, self.embed_dim])\n",
    "        return interest_capsule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "363da6eb-71a9-4fdd-9566-acb67bfa8495",
   "metadata": {},
   "outputs": [],
   "source": [
    "class fcn_net(tf.keras.layers.Layer):\n",
    "    def __init__(self,layer_sizes=[100,64]):\n",
    "        super(fcn_net, self).__init__()\n",
    "        self.hidden_layer={}\n",
    "        self.hidden_layer['layer_0']=tf.keras.layers.Dense(layer_sizes[0],activation='relu')\n",
    "        self.hidden_layer['layer_1']=tf.keras.layers.Dense(layer_sizes[1],activation='relu')\n",
    "        self.bn=[tf.keras.layers.BatchNormalization(\n",
    "                            momentum=0.95,\n",
    "                            epsilon=0.0001\n",
    "                    ),\n",
    "                 tf.keras.layers.BatchNormalization(\n",
    "                            momentum=0.95,\n",
    "                            epsilon=0.0001\n",
    "                    )]\n",
    "        self.last_out=tf.keras.layers.Dense(1)\n",
    "    def call(self, model_output, layer_sizes=[100,64], enable_BN=True):\n",
    "        last_layer_size = tf.shape(model_output)[-1]\n",
    "        layer_idx = 0\n",
    "        hidden_nn_layers = []\n",
    "        hidden_nn_layers.append(model_output)\n",
    "        for idx, layer_size in enumerate(layer_sizes):\n",
    "            curr_hidden_nn_layer=self.hidden_layer['layer_'+str(idx)](hidden_nn_layers[idx])\n",
    "            if enable_BN is True:\n",
    "                curr_hidden_nn_layer = self.bn[idx](curr_hidden_nn_layer)\n",
    "            hidden_nn_layers.append(curr_hidden_nn_layer)\n",
    "            layer_idx += 1\n",
    "            last_layer_size = layer_size\n",
    "        nn_output=self.last_out(hidden_nn_layers[-1])\n",
    "\n",
    "        return nn_output\n",
    "\n",
    "class attention_fcn(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(attention_fcn, self).__init__()\n",
    "        self.attention_mat = tf.keras.layers.Dense(hidden_size,use_bias=False)#[query_size]\n",
    "        \n",
    "        self._fcn_net=fcn_net(layer_sizes=[80,40])\n",
    "    def call(self, query, key_value, return_alpha=False):\n",
    "        att_inputs = self.attention_mat(key_value)\n",
    "        if query.shape.ndims != att_inputs.shape.ndims:\n",
    "            queries = tf.reshape(\n",
    "                tf.tile(query, [1, tf.shape(att_inputs)[1]]), tf.shape(att_inputs)\n",
    "            )\n",
    "        else:\n",
    "            queries = query\n",
    "        last_hidden_nn_layer = tf.concat(\n",
    "            [att_inputs, queries, att_inputs - queries, att_inputs * queries], -1\n",
    "        )\n",
    "\n",
    "        att_fnc_output = self._fcn_net(last_hidden_nn_layer)\n",
    "\n",
    "        att_fnc_output = tf.squeeze(att_fnc_output, -1)\n",
    "\n",
    "        att_weights = tf.nn.softmax(att_fnc_output\n",
    "        )\n",
    "\n",
    "        output = key_value * tf.expand_dims(att_weights, -1)\n",
    "        if not return_alpha:\n",
    "            return output\n",
    "        else:\n",
    "            return output, att_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "daf51b2b-c7ad-44aa-b691-93bc9533bc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bilinear(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(Bilinear, self).__init__()\n",
    "        self.linear_act = tf.keras.layers.Dense(units, activation=None, use_bias=True)\n",
    "        self.linear_noact = tf.keras.layers.Dense(units, activation=None, use_bias=False)\n",
    "\n",
    "    def call(self, a, b, gate_b=None):\n",
    "        if gate_b is None:\n",
    "            return tf.nn.sigmoid(self.linear_act(a) + self.linear_noact(b))\n",
    "        else:\n",
    "            return tf.nn.tanh(self.linear_act(a) + tf.math.multiply(gate_b, self.linear_noact(b)))\n",
    "\n",
    "class MIGRU(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        ###-------------------------------###\n",
    "\n",
    "    def call(self, inputs, state, att_score):\n",
    "        ###-------------------------------###\n",
    "        return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49fb9d25-6709-4545-9211-898760d749eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class mgnm(tf.keras.models.Model):\n",
    "    def __init__(self):\n",
    "        super(mgnm, self).__init__()\n",
    "        blocks=1\n",
    "        embed_reg=0.\n",
    "        layer_norm_eps=1e-6\n",
    "        num_heads=1\n",
    "        use_l2norm=False\n",
    "        \n",
    "        self.len_seq=len_seq\n",
    "        self.item_embedding = tf.keras.layers.Embedding(items_num+1,hidden_size,input_length=self.len_seq,\n",
    "                                                       embeddings_initializer='random_normal',\n",
    "                                                        embeddings_regularizer=tf.keras.regularizers.l2(embed_reg))\n",
    "        self.pos_embedding = tf.keras.layers.Embedding(len_seq,hidden_size,input_length=1,\n",
    "                                                       embeddings_initializer='random_normal',\n",
    "                                        embeddings_regularizer=tf.keras.regularizers.l2(embed_reg))\n",
    "        self.dropout = tf.keras.layers.Dropout(1-keep_rate)\n",
    "        self.hiera=hierarchical_interest(hidden_size,0)\n",
    "\n",
    "        self.multi_interest_dict={}\n",
    "        for i in range(0,2*(layers_num+1)):\n",
    "            self.multi_interest_dict['multi_interest_layer_'+str(i)+'_w1']=tf.keras.layers.Dense(hidden_size * 4, activation='tanh')\n",
    "            self.multi_interest_dict['multi_interest_layer_'+str(i)+'_w2']=tf.keras.layers.Dense(interest_num, activation=None)\n",
    "        \n",
    "        self.gru_encoder={}\n",
    "        self.MIGRU_encoder={}\n",
    "        self.attention_weight={}\n",
    "        for i in range(0,2*(layers_num+1)):\n",
    "            self.MIGRU_encoder['augru_encoder'+str(i)]=MIGRU(64)\n",
    "            self.gru_encoder['gru_encoder'+str(i)] = tf.keras.layers.GRU(hidden_size,return_sequences=True)\n",
    "            \n",
    "        self.gcn=gcn_layer(hidden_size, layers_num)\n",
    "        self.layer_weight=tf.Variable(initial_value=tf.ones((2*layers_num+1,1)),name='w_'+str(i-1),trainable=True)\n",
    "        # norm\n",
    "        self.use_l2norm = use_l2norm\n",
    "    \n",
    "    def calculate_score(self,att_outputs_dict,tar):\n",
    "        ###-------------------------------###\n",
    "        return score\n",
    "        \n",
    "        \n",
    "    \n",
    "    def call(self, inputs):\n",
    "        seq_embed=self.item_embedding(inputs['click_seq'])\n",
    "        ###-------------------------------###\n",
    "        all_embeddings=self.gcn(adj,seq_embed)\n",
    "        all_embeddings=self.hiera(all_embeddings)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        pos_encoding = tf.expand_dims(self.pos_embedding(tf.range(self.len_seq)), axis=0)  # (1, seq_len, embed_dim)\n",
    "        att_outputs_dict={}\n",
    "        \n",
    "        for i,element in enumerate(all_embeddings):\n",
    "            att_outputs_dict['embed_'+str(i)]=element+pos_encoding\n",
    "            att_outputs_dict['embed_'+str(i)] = self.dropout(att_outputs_dict['embed_'+str(i)])\n",
    "        \n",
    "        pos_emb=self.item_embedding(tf.reshape(inputs['pos_item'], [-1, ]))#[b,h]\n",
    "        pos_score=self.calculate_score(att_outputs_dict,pos_emb)\n",
    "        \n",
    "        if pred:\n",
    "            neg_indx=tf.random.uniform([tf.shape(inputs['pos_item'])[0],test_neg_num],dtype=tf.int32,maxval=items_num,minval=1)\n",
    "        else:\n",
    "            neg_indx=tf.random.uniform([tf.shape(inputs['pos_item'])[0],neg_num],dtype=tf.int32,maxval=items_num,minval=1)\n",
    "        \n",
    "        neg_emb=self.item_embedding(neg_indx)\n",
    "        neg_score=self.calculate_score(att_outputs_dict,neg_emb)\n",
    "        \n",
    "        logits = tf.nn.softmax(tf.concat([pos_score, neg_score], axis=-1))\n",
    "        \n",
    "        loss = tf.reduce_mean(- tf.math.log(logits)[:,0])+1e-6*self.adj_l1\n",
    "        self.add_loss(loss)\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "    def summary(self):\n",
    "        inputs = {\n",
    "            'click_seq': tf.keras.layers.Input(shape=(self.len_seq,), dtype=tf.int32),\n",
    "            'pos_item': tf.keras.layers.Input(shape=(), dtype=tf.int32)\n",
    "        }\n",
    "        tf.keras.models.Model(inputs=inputs, outputs=self.call(inputs)).summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab39a04b-df32-46da-93f0-7f2749da22d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hr(rank, k):\n",
    "    \"\"\"Hit Rate.\n",
    "    Args:\n",
    "        :param rank: A list.\n",
    "        :param k: A scalar(int).\n",
    "    :return: hit rate.\n",
    "    \"\"\"\n",
    "    res = 0.0\n",
    "    for r in rank:\n",
    "        if r < k:\n",
    "            res += 1\n",
    "    return res / len(rank)\n",
    "\n",
    "\n",
    "def mrr(rank, k):\n",
    "    \"\"\"Mean Reciprocal Rank.\n",
    "    Args:\n",
    "        :param rank: A list.\n",
    "        :param k: A scalar(int).\n",
    "    :return: mrr.\n",
    "    \"\"\"\n",
    "    mrr = 0.0\n",
    "    for r in rank:\n",
    "        if r < k:\n",
    "            mrr += 1 / (r + 1)\n",
    "    return mrr / len(rank)\n",
    "\n",
    "\n",
    "def ndcg(rank, k):\n",
    "    \"\"\"Normalized Discounted Cumulative Gain.\n",
    "    Args:\n",
    "        :param rank: A list.\n",
    "        :param k: A scalar(int).\n",
    "    :return: ndcg.\n",
    "    \"\"\"\n",
    "    res = 0.0\n",
    "    for r in rank:\n",
    "        if r < k:\n",
    "            res += 1 / np.log2(r + 2)\n",
    "    return res / len(rank)\n",
    "\n",
    "\n",
    "def eval_rank(pred_y, metric_names, k=10):\n",
    "    \"\"\"Evaluate\n",
    "        Args:\n",
    "            :param pred_y: A ndarray.\n",
    "            :param metric_names: A list like ['hr'].\n",
    "            :param k: A scalar(int).\n",
    "        :return: A result dict such as {'hr':, 'ndcg':, ...}\n",
    "    \"\"\"\n",
    "    rank = pred_y.argsort().argsort()[:, 0]\n",
    "    res_dict = {}\n",
    "    for name in metric_names:\n",
    "        if name == 'hr':\n",
    "            res = hr(rank, k)\n",
    "        elif name == 'ndcg':\n",
    "            res = ndcg(rank, k)\n",
    "        elif name == 'mrr':\n",
    "            res = mrr(rank, k)\n",
    "        elif name == 'precision':\n",
    "            res = hr(rank, 1)\n",
    "        else:\n",
    "            break\n",
    "        res_dict[name] = res\n",
    "    return res_dict\n",
    "\n",
    "def eval_pos_neg(model, test_data, metric_names, k=10, batch_size=None):\n",
    "    \"\"\"Evaluate the performance of Top-k recommendation algorithm.\n",
    "    Note: Test data must contain some negative samples(>= k) and one positive samples.\n",
    "    Args:\n",
    "        :param model: A model built-by tensorflow.\n",
    "        :param test_data: A dict.\n",
    "        :param metric_names: A list like ['hr'].\n",
    "        :param k: A scalar(int).\n",
    "        :param batch_size: A scalar(int).\n",
    "    :return: A result dict such as {'hr':, 'ndcg':, ...}\n",
    "    \"\"\"\n",
    "    \n",
    "    if len(test_data['users'])<30000:\n",
    "        pred_y = - model.predict(test_data, batch_size)\n",
    "        print(np.shape(pred_y))\n",
    "        return eval_rank(pred_y, metric_names, k)\n",
    "    else:\n",
    "        final_dict={}\n",
    "        for i in metric_names:\n",
    "            final_dict.update({i:[]})\n",
    "        for i in range(0,int(len(test_data['users'])/30000)+1):\n",
    "            part_test_data_users=test_data['users'][i*30000:(i+1)*30000]\n",
    "            part_test_data_poss=test_data['pos_item'][i*30000:(i+1)*30000]\n",
    "            part_test_data_cli=test_data['click_seq'][i*30000:(i+1)*30000]\n",
    "            part_test_data = {'users': np.array(part_test_data_users),'click_seq': np.array(part_test_data_cli),'pos_item': np.array(part_test_data_poss)}\n",
    "            pred_y = - model.predict(part_test_data, batch_size)\n",
    "            print(np.shape(pred_y))\n",
    "            part_dict=eval_rank(pred_y, metric_names, k)\n",
    "            for i in metric_names:\n",
    "                final_dict [i].append(part_dict[i])\n",
    "        for i in metric_names:\n",
    "            final_dict [i]=np.mean(final_dict [i])\n",
    "        return final_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26d2232c-3d17-4785-8889-ac02a07aec67",
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6e6348fc-1deb-4f08-8126-787466ee0424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Collective ops is not configured at program startup. Some performance features may not be enabled.\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:From C:\\Users\\Zhouziyue\\miniconda3\\envs\\rec\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py:5049: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "102/102 [==============================] - ETA: 0s - loss: 1.3126INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "102/102 [==============================] - 131s 503ms/step - loss: 1.3126 - val_loss: 1.2781\n",
      "Epoch 2/100\n",
      "102/102 [==============================] - 33s 321ms/step - loss: 0.9047 - val_loss: 1.1470\n",
      "Epoch 3/100\n",
      "102/102 [==============================] - 33s 324ms/step - loss: 0.8278 - val_loss: 1.0833\n",
      "Epoch 4/100\n",
      "102/102 [==============================] - 33s 323ms/step - loss: 0.7781 - val_loss: 1.0414\n",
      "Epoch 5/100\n",
      "102/102 [==============================] - 33s 324ms/step - loss: 0.7491 - val_loss: 1.0300\n",
      "Epoch 6/100\n",
      "102/102 [==============================] - 34s 328ms/step - loss: 0.7176 - val_loss: 1.0203\n",
      "Epoch 7/100\n",
      "102/102 [==============================] - 36s 349ms/step - loss: 0.7019 - val_loss: 1.0200\n",
      "Epoch 8/100\n",
      "102/102 [==============================] - 33s 320ms/step - loss: 0.6830 - val_loss: 1.0268\n",
      "Epoch 9/100\n",
      "102/102 [==============================] - 33s 323ms/step - loss: 0.6649 - val_loss: 1.0232\n",
      "Epoch 10/100\n",
      "102/102 [==============================] - 33s 322ms/step - loss: 0.6493 - val_loss: 1.0305\n",
      "(14264, 1001)\n",
      "K=5, Precision = 0.0146, HR = 0.0611, MRR = 0.0297, NDCG = 0.0374\n",
      "(14264, 1001)\n",
      "K=10, Precision = 0.0154, HR = 0.1092, MRR = 0.0364, NDCG = 0.0531\n",
      "(14264, 1001)\n",
      "K=20, Precision = 0.0141, HR = 0.1908, MRR = 0.0409, NDCG = 0.0727\n",
      "K=5, Fit [469.0 s]: Precision = 0.0146, HR = 0.0611, MRR = 0.0297, NDCG = 0.0374\n",
      "K=10, Fit [469.0 s]: Precision = 0.0154, HR = 0.1092, MRR = 0.0364, NDCG = 0.0531\n",
      "K=20, Fit [469.0 s]: Precision = 0.0141, HR = 0.1908, MRR = 0.0409, NDCG = 0.0727\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "from tensorflow.keras.backend import clear_session\n",
    "number=1\n",
    "max_layers=10\n",
    "k=[5,10,20]\n",
    "ave_dict_list=[{'hr':[],'mrr':[],'ndcg':[],'precision':[]} for _ in range(len(k))]\n",
    "t1 = time()\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "batch_size=batch_size*strategy.num_replicas_in_sync\n",
    "for i in range(number):\n",
    "    clear_session()\n",
    "    with strategy.scope():\n",
    "        pred=0\n",
    "        indx=1\n",
    "        model = mgnm()\n",
    "        #model.summary()\n",
    "        optimizer=tf.keras.optimizers.Nadam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004)\n",
    "        model.compile(optimizer=optimizer)\n",
    "        model.fit(\n",
    "                x=train_data,\n",
    "                epochs=epoch_num,\n",
    "                verbose=1,\n",
    "                batch_size=batch_size,\n",
    "            callbacks=callback,\n",
    "            validation_data=val_data)\n",
    "        \n",
    "    pred=1\n",
    "    indx=0\n",
    "    for idx,i in enumerate(k):\n",
    "        eval_dict = eval_pos_neg(model, test_data, ['hr', 'mrr', 'ndcg','precision'], i, batch_size)\n",
    "        for m in ['hr', 'mrr', 'ndcg','precision']:\n",
    "            ave_dict_list[idx][m].append(eval_dict[m])\n",
    "        print('K=%d, Precision = %.4f, HR = %.4f, MRR = %.4f, NDCG = %.4f'\n",
    "              % (i, eval_dict['precision'], eval_dict['hr'], eval_dict['mrr'], eval_dict['ndcg']))\n",
    "t2 = time()\n",
    "for idx,i in enumerate(k):\n",
    "    print('K=%d, Fit [%.1f s]: Precision = %.4f, HR = %.4f, MRR = %.4f, NDCG = %.4f'\n",
    "              % (i, t2 - t1, np.mean(ave_dict_list[idx]['precision']),np.mean(ave_dict_list[idx]['hr']), np.mean(ave_dict_list[idx]['mrr']), np.mean(ave_dict_list[idx]['ndcg'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9fc673-c477-4950-bce9-456cf20b4f6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
